{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение для текстов. Проект"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Общее-впечатление\" data-toc-modified-id=\"Общее-впечатление-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span><font color=\"orange\">Общее впечатление</font></a></span></li><li><span><a href=\"#Общее-впечатление-(ревью-2)\" data-toc-modified-id=\"Общее-впечатление-(ревью-2)-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span><font color=\"orange\">Общее впечатление (ревью 2)</font></a></span></li></ul></li><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучим модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Построим модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "Данный проект также выполнен локально с BERT и Catboost text_features:\n",
    "- BERT: https://drive.google.com/file/d/1sIqWh-Hty9FOBba7GqbMcy21je8xsof_/view?usp=sharing\n",
    "- CatBoost: https://drive.google.com/file/d/1V31q_SRPvgMzkbQlpDoyjr-_aE6SwX9s/view?usp=sharing\n",
    "\n",
    "\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "0       Explanation\\nWhy the edits made under my usern...      0\n",
       "1       D'aww! He matches this background colour I'm s...      0\n",
       "2       Hey man, I'm really not trying to edit war. It...      0\n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4       You, sir, are my hero. Any chance you remember...      0\n",
       "...                                                   ...    ...\n",
       "159566  \":::::And for the second time of asking, when ...      0\n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0\n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0\n",
       "159569  And it looks like it was actually you who put ...      0\n",
       "159570  \"\\nAnd ... I really don't think you understand...      0\n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим признаки и целевой признак перед обучением и разделим выборки.\n",
    "Создадим корпус текстов и вычислим для него TF-IDF с помощью TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 12.6 s, total: 1min 21s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = text.lower()\n",
    "    lemm_text = \"\".join(m.lemmatize(text))\n",
    "    cleared_text = re.sub(r'[^a-zA-Z]', ' ', lemm_text) \n",
    "    return \" \".join(cleared_text.split())\n",
    "\n",
    "df['lemm_text'] = df['text'].apply(lemmatize_text)\n",
    "\n",
    "df = df.drop(['text'], axis=1)\n",
    "del m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['toxic']\n",
    "features = df.drop(['toxic'], axis=1)\n",
    "\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, \n",
    "                                                                              target, \n",
    "                                                                              test_size=0.4, \n",
    "                                                                              random_state=12082020)\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(features_valid, \n",
    "                                                                            target_valid, \n",
    "                                                                            test_size=0.5,\n",
    "                                                                            random_state=12082020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95742, 126292)\n",
      "(31914, 126292)\n",
      "(31915, 126292)\n"
     ]
    }
   ],
   "source": [
    "features_train = count_tf_idf.fit_transform(features_train['lemm_text'].values.astype('U'))\n",
    "features_valid = count_tf_idf.transform(features_valid['lemm_text'].values.astype('U'))\n",
    "features_test = count_tf_idf.transform(features_test['lemm_text'].values.astype('U'))\n",
    "print(features_train.shape)\n",
    "print(features_valid.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения выберем LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for f1_score\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'class_weight': 'balanced', 'solver': 'liblinear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.701968 for {'C': 0.1, 'class_weight': 'balanced', 'solver': 'newton-cg'}\n",
      "0.701968 for {'C': 0.1, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "0.701905 for {'C': 0.1, 'class_weight': 'balanced', 'solver': 'liblinear'}\n",
      "0.361926 for {'C': 0.1, 'class_weight': 'none', 'solver': 'newton-cg'}\n",
      "0.361790 for {'C': 0.1, 'class_weight': 'none', 'solver': 'lbfgs'}\n",
      "nan for {'C': 0.1, 'class_weight': 'none', 'solver': 'liblinear'}\n",
      "0.746687 for {'C': 1, 'class_weight': 'balanced', 'solver': 'newton-cg'}\n",
      "0.746687 for {'C': 1, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "0.746746 for {'C': 1, 'class_weight': 'balanced', 'solver': 'liblinear'}\n",
      "0.673228 for {'C': 1, 'class_weight': 'none', 'solver': 'newton-cg'}\n",
      "0.673228 for {'C': 1, 'class_weight': 'none', 'solver': 'lbfgs'}\n",
      "nan for {'C': 1, 'class_weight': 'none', 'solver': 'liblinear'}\n",
      "0.763875 for {'C': 10, 'class_weight': 'balanced', 'solver': 'newton-cg'}\n",
      "0.763626 for {'C': 10, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "0.763938 for {'C': 10, 'class_weight': 'balanced', 'solver': 'liblinear'}\n",
      "0.747660 for {'C': 10, 'class_weight': 'none', 'solver': 'newton-cg'}\n",
      "0.747887 for {'C': 10, 'class_weight': 'none', 'solver': 'lbfgs'}\n",
      "nan for {'C': 10, 'class_weight': 'none', 'solver': 'liblinear'}\n",
      "\n",
      "CPU times: user 10min 23s, sys: 11min 53s, total: 22min 17s\n",
      "Wall time: 22min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classificator = LogisticRegression()\n",
    "hyperparams = [{'solver':['newton-cg', 'lbfgs', 'liblinear'],\n",
    "                'C':[0.1, 1, 10],\n",
    "                'class_weight':['balanced', 'none']}]\n",
    "\n",
    "\n",
    "print('# Tuning hyper-parameters for f1_score')\n",
    "print()\n",
    "clf = GridSearchCV(classificator, hyperparams, scoring='f1',cv=3)\n",
    "clf.fit(features_train, target_train)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "LR_best_params = clf.best_params_\n",
    "print(LR_best_params)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.6f for %r\"% (mean, params))\n",
    "print()\n",
    "\n",
    "cv_f1_LR = max(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на cv 0.7639381490898353\n",
      "F1 на валидации 0.7675818528850389\n",
      "CPU times: user 14 s, sys: 13.2 s, total: 27.3 s\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classificator = LogisticRegression()\n",
    "classificator.set_params(**LR_best_params)\n",
    "classificator.fit(features_train, target_train)\n",
    "target_predict = classificator.predict(features_valid)\n",
    "valid_f1_LR = f1_score(target_valid, target_predict)\n",
    "print('F1 на cv', cv_f1_LR)\n",
    "print('F1 на валидации', valid_f1_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.7614080834419819\n"
     ]
    }
   ],
   "source": [
    "classificator = LogisticRegression()\n",
    "classificator.set_params(**LR_best_params)\n",
    "classificator.fit(features_train, target_train)\n",
    "predict_test = classificator.predict(features_test)\n",
    "print('F1:', f1_score(target_test, predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговые F1 на других моделях:\n",
    "- LR с BERT: https://drive.google.com/file/d/1sIqWh-Hty9FOBba7GqbMcy21je8xsof_/view?usp=sharing\n",
    "- CatBoost: https://drive.google.com/file/d/1V31q_SRPvgMzkbQlpDoyjr-_aE6SwX9s/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Результаты проверки обученных моделей на тестовой выборке:\n",
    "  - LogisticRegression на подготовленных признаках с помощью TF-IDF: **0.761**\n",
    "  - LogisticRegression на подготовленных признаках с помощью BERT: **0.722**\n",
    "  - CatBoost на подготовленных признаках с помощью text_features: **0.783**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следует отметить, что подготовка признаков с помощью BERT занимает на локальной видеокарте ~1,5 часа, на видеокарте Colab ~3 часа. В случаях LogisticRegression TF-IDF вычисление проекта заняло ~30 мин, а CatBoost text_features - всего ~5 мин."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод** - лучшая модель CatBoost на подготовленных признаках с помощью text_features"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1975,
    "start_time": "2022-05-05T17:48:26.194Z"
   },
   {
    "duration": 940,
    "start_time": "2022-05-05T17:49:18.773Z"
   },
   {
    "duration": 2463,
    "start_time": "2022-05-10T11:56:07.029Z"
   },
   {
    "duration": 1029,
    "start_time": "2022-05-10T11:56:09.495Z"
   },
   {
    "duration": 10,
    "start_time": "2022-05-10T11:56:10.526Z"
   },
   {
    "duration": 176695,
    "start_time": "2022-05-10T11:57:17.704Z"
   },
   {
    "duration": 82,
    "start_time": "2022-05-10T12:02:55.502Z"
   },
   {
    "duration": 150,
    "start_time": "2022-05-10T12:02:57.339Z"
   },
   {
    "duration": 17323,
    "start_time": "2022-05-10T12:03:12.540Z"
   },
   {
    "duration": 290,
    "start_time": "2022-05-10T12:05:00.678Z"
   },
   {
    "duration": 122,
    "start_time": "2022-05-10T12:05:01.309Z"
   },
   {
    "duration": 1944,
    "start_time": "2022-05-10T12:06:08.024Z"
   },
   {
    "duration": 1339683,
    "start_time": "2022-05-10T12:06:20.582Z"
   },
   {
    "duration": 27314,
    "start_time": "2022-05-10T12:28:40.268Z"
   },
   {
    "duration": 27231,
    "start_time": "2022-05-10T12:29:15.866Z"
   },
   {
    "duration": 25435,
    "start_time": "2022-05-10T12:34:51.516Z"
   },
   {
    "duration": 26231,
    "start_time": "2022-05-10T12:35:33.140Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
